<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Chunyou Peng Blog</title>
    <link>http://example.org/posts/</link>
    <description>Recent content in Posts on Chunyou Peng Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 06 Apr 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://example.org/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>My Master Theorem Understanding</title>
      <link>http://example.org/posts/master-theorem/</link>
      <pubDate>Sat, 06 Apr 2024 00:00:00 +0000</pubDate>
      <guid>http://example.org/posts/master-theorem/</guid>
      <description>Book Screenshot My Quick Understanding 就是比较f(n)和$n^{\log{b^a}}$&#xA;相同,则是log f(n)的阶数要大一点,T(n)=O($n^{\log{b^a}}$) f(n)的阶数要小一点,要判断一个af(n/b) &amp;lt;= cf(n),c&amp;lt;1.如果满足,T(n)=O(f(n)) </description>
    </item>
    <item>
      <title>怎么避免心脏病</title>
      <link>http://example.org/posts/heart-issues/</link>
      <pubDate>Tue, 26 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://example.org/posts/heart-issues/</guid>
      <description>产生心脏问题的原因 长期压力大 -&amp;gt; 肾上腺素升高 -&amp;gt; 会产生大量的糖 -&amp;gt; 增加固醇 -&amp;gt; 心脏缺氧&#xA;解决方法 减少压力 可以选择在大自然中散步,会降低肾上腺素.冥想.把心态放好.&#xA;饮食 不要吃糖 日常吃B1 维B可以控制乳酸</description>
    </item>
    <item>
      <title>DetectGPT Zero-Shot Machine-Generated Text Detection using Probability Curvature</title>
      <link>http://example.org/posts/detectgpt/</link>
      <pubDate>Mon, 25 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://example.org/posts/detectgpt/</guid>
      <description>Problem 目前，使用ai生成文本已经非常常见了。但是目前并没有什么好的方法可以很好的检测ai生成的文本。&#xA;Assumption in prior work Prior work trains a second model to detect machine-gennerated text, but it may overfit the topic.&#xA;Insight They consider the zero-shot version of machine-generated text detection by evaluating the average per-token log probability of the generated text and thresholding.&#xA;Technical overview Proof Impact </description>
    </item>
    <item>
      <title>Exploring Document-Level Literary Machine Translation with Parallel Paragraphs from World Literature</title>
      <link>http://example.org/posts/document-level-mt/</link>
      <pubDate>Mon, 25 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://example.org/posts/document-level-mt/</guid>
      <description>Problem While the improvement of the machine translation(MT) recently, the ablitiy to automatically translate literary text is limited.&#xA;Assumption in prior work Insight They leverage large pretrained language models to improve literary MT and introduce their large-scale dataset PAR3.&#xA;Technical overview The PAR 3 Dataset: Parallel Paragraph-Level Paraphrases PAR 3 was curated in four stages: selec-tion of source texts, machine translation of source texts, paragraph alignment, and final filtering.&#xA;它至少有两种以上的人工翻译可以对照，并且有一个Google翻译本。人工翻译和Google翻译之间需要对齐。并且把短的句子删掉。</description>
    </item>
    <item>
      <title>怎么解决个从痘痘问题</title>
      <link>http://example.org/posts/acne-problem/</link>
      <pubDate>Mon, 25 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://example.org/posts/acne-problem/</guid>
      <description>彻底解决痘痘的问题 原因 控制不住用手去摸 因为脸上很痒 辣的，糖类，蛋糕，油炸，牛奶，汉堡，零食，饮料不要吃 思考问题的时候如果手没有放处也会想扣脸 root我很容易分心(1) PLAN(1) 让手不能闲下来 留意自己手的位置，如果发现超出自己的视线之外，立马反应过来。 每天冥想10分钟 不吃蔬菜水果 蔬菜很难吃(2) PLAN(2) 食堂吃饭,如果有花菜,卷心菜,洋葱, 胡萝卜,一定要优先选. 吃水果很麻烦，要剥皮，要扔垃圾，还要洗手 在美团上直接买处好的水果 可以在看海贼王看视频的时候拿出水果来吃. 晚上两点以后睡觉 总觉得事情没有做完，书还没看，视频还没看(3) PLAN(3) 到了晚上12点,就不要去看YouTube,Bilibili, Tiktok这些网站了. 不要看山鹰哥了.直接打开Podcast,开睡. root我很焦虑 I&amp;rsquo;m nervous for my career(4) PLAN(4) 心里暗示,我能行. 稳定的进步比快速的进步更好. 每天把最要的事情列出来,写在一个清单里, 晚上看看, 只要做了其中的一件,令天就算可以了. 做事的时候,把大问题拆分成小问题去解决,就能更有掌控感,不会那么迷茫和焦虑. 不运动，不出汗 I&amp;rsquo;m lazy 浑身无力 PLAN 就从骑自行车开始. 并且没有养成运动的习惯 晚上8:30去b跑5公里. 跑完之后做拉伸10分钟. 没人的时候要用英语流利说的同时拉伸. 时候一个小时. 目前的进展是要用手去用力往下拉了, 身体往前靠.第三个阶段了.第4个阶段是用手裹住脚尖.第5个阶段是脚悬空或者不行的话,用布裹着悬空. TODO 人时候会忘记抹护肤品 I&amp;rsquo;m forgettable 并且不够重视 每天洗两次脸,吃了中午饭一次, 晚上一次. 洗脸之后就抺凝胶-&amp;gt;抺保湿. 吃的东西全是肉，而且很油腻 炸肉很好吃，吃得很爽 root我是肉食动物，喜欢大块吃肉，目前能大块吃肉的也就为些汉堡炸鸡了 点外卖的时候只点稀饭. </description>
    </item>
    <item>
      <title>Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense</title>
      <link>http://example.org/posts/retrieval-detectors/</link>
      <pubDate>Sun, 24 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://example.org/posts/retrieval-detectors/</guid>
      <description>Problem Assumption in prior work Insight Technical overview Proof Impact </description>
    </item>
    <item>
      <title>Multi-Concept Customization of Text-to-Image Diffusion</title>
      <link>http://example.org/posts/customized-text-imge/</link>
      <pubDate>Sat, 23 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://example.org/posts/customized-text-imge/</guid>
      <description>Problem Users wish to synthesize specific concepts from their own personal lives.用户想要定制自己的主题。但是现有的模型很难产生未看到的背景。&#xA;Assumption in prior work Prior work improves compositional generation to extend beyond tuning for a single, individual concept and compose multiple conceots together, but they lead addtional chanllenges as mixing unseen concepts.&#xA;Insight 他们识别出了小部份的模型权重，通过稍调这些小部分的权重就足够更新模型适应新的图像概念。 augmentation 他们提出的方法可以分别训练然后合并。&#xA;Technical overview 该方法支持训练多个概念并在新颖的环境中将它们组合起来，同时生成现有概念的新变体。该方法优于几个基准和同时进行的工作，在定性和定量评估中表现出色，同时具有计算和内存效率。&#xA;Proof 结果比较 单概念微调结果比较&#xA;他们的模型可以生成背景和艺术风格。以第一行为例，生成一个水彩风格的在山上的乌龟。他们的模型能产生山的背景，而其它的比如DreamBooth和Textual Inversion则不行，效果较差。&#xA;多概念微调结果比较&#xA;在多概念的情况下，也就是把多个主题融合起来。从上图可以看到，他们的模型有更高的视觉相似度（两个或多个视觉对象在视觉特征上的相似度）。然而DreamBooth有时会忽略掉猫。&#xA;量化比较 上面的结果表明，不管是单概念还是多概念都要优于DreamBooth&#xA;本文提出了一种在新概念、类别、个人对象或艺术风格上微调大规模文本到图像扩散模型的方法，只使用少量图像示例。该方法在新环境中生成微调概念的新变体，保持与目标图像的视觉相似性。它还可以在同一场景中组合多个新概念，并在背景中生成山脉。该方法在每个组合设置的8个提示生成的400个图像上优于其他基准。人类偏好研究表明，所提出的方法在单一概念和多概念方面优于基准。该方法在难以组合的情况下显示出局限性，例如宠物狗和猫，以及组合三个或更多概念。&#xA;Impact 他们提出了种新方法用来微调大规模文生图的扩散模型，只需要几个图片就可以生成新的概念，主题和艺术风格。他们的优点如下：&#xA;高效计算 只需要小部份的权重就可以进行训练 </description>
    </item>
    <item>
      <title>Large Language Models Struggle to Learn Long-Tail Knowledge</title>
      <link>http://example.org/posts/long-tail/</link>
      <pubDate>Fri, 22 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://example.org/posts/long-tail/</guid>
      <description>Problem Assumption in prior work Insight Technical overview Proof Impact </description>
    </item>
    <item>
      <title>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</title>
      <link>http://example.org/posts/rag/</link>
      <pubDate>Fri, 22 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://example.org/posts/rag/</guid>
      <description>Problem Pre-trained neural language models cannot expand their memory and may produce &amp;ldquo;hallucinations&amp;rdquo;, which limits the capability of such models.&#xA;Assumptin in prior work Hybrid models that combines parametric memory with non-parametric momories solve some problems. However, these models have only explored open-domain extractive question answering.&#xA;Prior work enrich systems with non-parametric memory by training from scratch for specific tasks. In contrast, all their components are pre-trained.&#xA;Insight They endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose fine-tuning approach as retrieval-augmented generation(RAG).</description>
    </item>
    <item>
      <title>How to Write a Paper?</title>
      <link>http://example.org/posts/writing-papers/</link>
      <pubDate>Wed, 20 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://example.org/posts/writing-papers/</guid>
      <description>How to Get Started? First, find the nearest paper, then its neighbors. Give them topics to write your liturature review. Find what they have done, and what they didn&amp;rsquo;t.&#xA;How to read a paper Outlining a Paper Your outline should recompress the paper back into an outline format that explains the paper&amp;rsquo;s argument. We suggest the following structure of one paragraph per bullet, with an example outline following the description:</description>
    </item>
    <item>
      <title>Notes of Boosting Monocular Depth Estimation Models to High-Resolution via Content-Adaptive Multi-Resolution Merging</title>
      <link>http://example.org/posts/depth-estimation/</link>
      <pubDate>Wed, 20 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://example.org/posts/depth-estimation/</guid>
      <description>研究内容 单个相机拍出来的照片是很难估计它的深度的.目前主要流行用神经网络解决这个问题.但是要得到高清晰度和一个好的场景结构是很难的.目前最好的方法是全卷积架构.但受限于GPU,数据集,CNN接受域的大小.&#xA;他们观察到，随着图片的清析度越高，模型的结构越差，但是细节表现更好。他们的方法是采用一个预训练单深度估计模型实现高分辨率和高精度边缘检测.采用双估计框架合并两个图片可以得到较好的效果.第二个是从图象里面选出块放到模型里面然后再合并.&#xA;研究方法 (b) We first start with feeding the image in low- and high-resolution to the network, here shown results with MiDaS [34], and merge them to get a base estimate with a consistent structure with good boundary localization. (c) We then determine different patches in the image. We show a subset of selected patches with their depth estimates. (d) We merge the patch estimates onto our base estimate from (b) to get our final high-resolution result.</description>
    </item>
    <item>
      <title>Few-Shot Prompting Engeering</title>
      <link>http://example.org/posts/fewshot-prompting/</link>
      <pubDate>Sun, 03 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://example.org/posts/fewshot-prompting/</guid>
      <description>Few-Shot LLM Summary Summarize the papers in this section. You should not simply list the reviewed papers. A good practice is to discuss the connections between them, e.g., paper 1 proposes a xxx method, which solves xxx problems, but it cannot do xxx; while paper 2 improves paper 1’s theory/algorithm/approach/dataset, and it can solve xxx problems, and thus it can achieve xxx effect. But there are still some other limitations, e.</description>
    </item>
    <item>
      <title>My Personal TODO App</title>
      <link>http://example.org/posts/todo-app/</link>
      <pubDate>Sat, 02 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://example.org/posts/todo-app/</guid>
      <description>TODO-APP 需求 软件思路 这个app用来管理用户的todo事项。&#xA;用户可以添加，修改，删除事项。每个事项有一个优先级，1－5星。优先级最高的排在最前面。用户点击事项前面的已完成按扭之后，就会被移到已完成的一个箱子里面。用户可以选择点击这个箱子查看。&#xA;前端 前端使用vite 创建react项目.使用ts.css使用tailwincss&#xA;按扭有添加,删除,已完成,设定优先级(1-5).完成箱(用来查看已完成的事项) 并且显示添加的日期&#xA;后端 数据库使用postgresql&#xA;内容有 完成情况(已完成/未完成) | 事件 | 优先级 | 添加的日期</description>
    </item>
    <item>
      <title>Personal Wrods DB App</title>
      <link>http://example.org/posts/words-db/</link>
      <pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://example.org/posts/words-db/</guid>
      <description>Personal Wrods DB App 需求 软件思路 这个软件的目的是记录和跟踪用户的词汇情况.&#xA;用户在日常的生活中,可能会遇到一些生词,这个时候就可以把这个单词录入到系统里面.系统首先在数据库里查询这个单词.如果有,则返回查询这个单词的次数和意思.如果没有,则使用一个函数 GetMeaning(word){return word + &amp;ldquo;meaning is&amp;rdquo;;}, 目前就一个定死的函数.返回他的意思,查询次数设为1.&#xA;系统的第二个功能是:用户在读一篇文本的时候,想知道哪些单词不认识.他可以把这篇文章复制到另一个查询框.系统通过查找每一个单词, 如果不在数库里面,系统一行一行输出这些单词的原单词,查询次数,单词意思.&#xA;前端 使用reactjs.有两个选项, 一个是录单词的.另一个就是录文本的.&#xA;下面都有对应的按纽,点击就调用具体的功能.&#xA;后端 数据库使用postgresql.&#xA;内容有 单词 | 查询次数 | 单词意思</description>
    </item>
    <item>
      <title>System of systems Technology Integration Tool Chain for Heterogeneous Electronic System</title>
      <link>http://example.org/posts/stitches/</link>
      <pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://example.org/posts/stitches/</guid>
      <description>背景 美国国防部(DoD)在过去有大量的不同数据标准、协议的系统。STITCHES就是为了实现系统在不需要共享公共标准的情况下实现实现互操作的一个系统.而子系统可以根据自已的需求独立开发而不受影响.&#xA;节点的传输 Transforms Expressed in a Domain Specific Language Built for this Purpose&#xA;点节的数据的传输是由定义在它上面的DSL进行的.&#xA;结构图 中间有一部是把配置文件转换成C++/Java代码,这一步就有我们做的非常相似.&#xA;DSL 提供高层的函数, 嵌套的表达式, 语法糖 提供了类型系统 辑写人可以直接读的语言 Config file 把上面的配置文件转换成对应的代码.</description>
    </item>
    <item>
      <title>优先级决定决策</title>
      <link>http://example.org/posts/priority/</link>
      <pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://example.org/posts/priority/</guid>
      <description>优先级 人是由priority驱动的。健身好不好？好。早睡好不好？好。但是为什么不去做？因为这些事在你心中的优先级还不够高。(比如叫一个人做事,他没有做.给一个人发消息,他没有回,原因只有一个, 就是你在他心中的优先级不够高.)&#xA;对于我而言，目前最高的优先级应该是健康。首先是身体的健康，心理的健康，再到心灵的健康。&#xA;我更关注的身体的健康包含哪些？&#xA;皮肤的健康(不要有痘) 头发的健康(不要掉发,要保持浓密的发量) 内脏的健康(心脏要保持正常跳动,肝,肾都要强壮) 眼睛的健康(不要在光线暗的地方看手机,特别是侧躺着看) 牙齿的健康(不要吃过冷,过热的东西,还有糖.每天必须保持牙齿的清洁) 听力的健康(能不戴耳机就不戴耳机) 腰腹的健康(要有力量) 生命就应该是最重要的, 健康就应该是最重要的. 不应该有任何的事情优先级大于这两个. 如果有一个赚钱升职的机会摆在面前, 但是代价是健康, 那就直接放弃.</description>
    </item>
    <item>
      <title>Docker Proxy Settings</title>
      <link>http://example.org/posts/docker-proxy/</link>
      <pubDate>Sat, 18 Mar 2023 00:00:00 +0000</pubDate>
      <guid>http://example.org/posts/docker-proxy/</guid>
      <description>Docker Proxy Docker build docker build --no-cache -t test:latest . \ --network host \ --build-arg HTTP_PROXY=http://127.0.0.1:7890 \ --build-arg HTTPS_PROXY=http://127.0.0.1:7890 Note I add &amp;ndash;no-cache flag</description>
    </item>
    <item>
      <title>2rd post</title>
      <link>http://example.org/posts/second-post/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>http://example.org/posts/second-post/</guid>
      <description>My Second posts </description>
    </item>
    <item>
      <title>Hello, world!</title>
      <link>http://example.org/posts/first-post/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>http://example.org/posts/first-post/</guid>
      <description>Hello World! This is my blog.</description>
    </item>
    <item>
      <title></title>
      <link>http://example.org/posts/%E9%95%BF%E5%87%BA%E6%B5%93%E5%AF%86%E7%9A%84%E5%A5%BD%E5%A4%B4%E5%8F%91/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://example.org/posts/%E9%95%BF%E5%87%BA%E6%B5%93%E5%AF%86%E7%9A%84%E5%A5%BD%E5%A4%B4%E5%8F%91/</guid>
      <description>怎么不掉发?并且长出好头发? ANDREW HUBERMAN: Welcome to the Huberman Lab podcast, where we discuss science and science -based tools for everyday life. [MUSIC PLAYING] I&amp;rsquo;m Andrew Huberman, and I&amp;rsquo;m a professor of neurobiology and ophthalmology at Stanford School of Medicine. Today, we are discussing hair. Hair is a topic that occupies the minds of many people. There are people that are losing their hair and want to halt or reverse that loss of hair.</description>
    </item>
  </channel>
</rss>
