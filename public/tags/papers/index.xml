<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Papers on Chunyou Peng Blog</title>
    <link>http://example.org/tags/papers/</link>
    <description>Recent content in Papers on Chunyou Peng Blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Mon, 25 Mar 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://example.org/tags/papers/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Exploring Document-Level Literary Machine Translation with Parallel Paragraphs from World Literature</title>
      <link>http://example.org/posts/document-level-mt/</link>
      <pubDate>Mon, 25 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://example.org/posts/document-level-mt/</guid>
      <description>Problem While the improvement of the machine translation(MT) recently, the ablitiy to automatically translate literary text is limited.&#xA;Assumption in prior work Insight They leverage large pretrained language models to improve literary MT and introduce their large-scale dataset PAR3.&#xA;Technical overview The PAR 3 Dataset: Parallel Paragraph-Level Paraphrases PAR 3 was curated in four stages: selec-tion of source texts, machine translation of source texts, paragraph alignment, and final filtering.&#xA;它至少有两种以上的人工翻译可以对照，并且有一个Google翻译本。人工翻译和Google翻译之间需要对齐。并且把短的句子删掉。</description>
    </item>
    <item>
      <title>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</title>
      <link>http://example.org/posts/rag/</link>
      <pubDate>Fri, 22 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://example.org/posts/rag/</guid>
      <description>Problem Pre-trained neural language models cannot expand their memory and may produce &amp;ldquo;hallucinations&amp;rdquo;, which limits the capability of such models.&#xA;Assumptin in prior work Hybrid models that combines parametric memory with non-parametric momories solve some problems. However, these models have only explored open-domain extractive question answering.&#xA;Prior work enrich systems with non-parametric memory by training from scratch for specific tasks. In contrast, all their components are pre-trained.&#xA;Insight They endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose fine-tuning approach as retrieval-augmented generation(RAG).</description>
    </item>
    <item>
      <title>How to Write a Paper?</title>
      <link>http://example.org/posts/writing-papers/</link>
      <pubDate>Wed, 20 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://example.org/posts/writing-papers/</guid>
      <description>How to Get Started? First, find the nearest paper, then its neighbors. Give them topics to write your liturature review. Find what they have done, and what they didn&amp;rsquo;t.&#xA;How to read a paper Outlining a Paper Your outline should recompress the paper back into an outline format that explains the paper&amp;rsquo;s argument. We suggest the following structure of one paragraph per bullet, with an example outline following the description:</description>
    </item>
    <item>
      <title>Few-Shot Prompting Engeering</title>
      <link>http://example.org/posts/fewshot-prompting/</link>
      <pubDate>Sun, 03 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://example.org/posts/fewshot-prompting/</guid>
      <description>Few-Shot LLM Summary Summarize the papers in this section. You should not simply list the reviewed papers. A good practice is to discuss the connections between them, e.g., paper 1 proposes a xxx method, which solves xxx problems, but it cannot do xxx; while paper 2 improves paper 1’s theory/algorithm/approach/dataset, and it can solve xxx problems, and thus it can achieve xxx effect. But there are still some other limitations, e.</description>
    </item>
  </channel>
</rss>
